[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "DANL Project",
    "section": "",
    "text": "About this project üëè\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "project.html#summary-statistics",
    "href": "project.html#summary-statistics",
    "title": "DANL Project",
    "section": "2.1 Summary Statistics",
    "text": "2.1 Summary Statistics\n\nmpg &lt;- ggplot2::mpg\n\n\n\n\n  \n\n\n\nskim(mpg) %&gt;% \n  select(-n_missing)\n\n\nData summary\n\n\nName\nmpg\n\n\nNumber of rows\n234\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\nskim_variable\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmanufacturer\n1\n4\n10\n0\n15\n0\n\n\nmodel\n1\n2\n22\n0\n38\n0\n\n\ntrans\n1\n8\n10\n0\n10\n0\n\n\ndrv\n1\n1\n1\n0\n3\n0\n\n\nfl\n1\n1\n1\n0\n5\n0\n\n\nclass\n1\n3\n10\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndispl\n1\n3.47\n1.29\n1.6\n2.4\n3.3\n4.6\n7\n‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n\n\nyear\n1\n2003.50\n4.51\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\ncyl\n1\n5.89\n1.61\n4.0\n4.0\n6.0\n8.0\n8\n‚ñá‚ñÅ‚ñá‚ñÅ‚ñá\n\n\ncty\n1\n16.86\n4.26\n9.0\n14.0\n17.0\n19.0\n35\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ\n\n\nhwy\n1\n23.44\n5.95\n12.0\n18.0\n24.0\n27.0\n44\n‚ñÖ‚ñÖ‚ñá‚ñÅ‚ñÅ"
  },
  {
    "objectID": "project.html#mpg-and-a-type-of-cars",
    "href": "project.html#mpg-and-a-type-of-cars",
    "title": "DANL Project",
    "section": "2.2 MPG and a Type of Cars",
    "text": "2.2 MPG and a Type of Cars\nThe following boxplot shows how the distribution of highway MPG (hwy) varies by a type of cars (class) üöô üöö üöê.\n\nggplot(data = mpg) +\n  geom_boxplot(aes(x = class, y = hwy, fill = class),\n               show.legend = F) +\n  labs(x = \"Class\", y = \"Highway\\nMPG\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Laya Davis",
    "section": "",
    "text": "Hi! I am Laya Davis, an Applied Mathematics Major at SUNY Geneseo. I am a part of SUNY Geneseo‚Äôs actuary program and am currently studying for Exam P and have passed Exam FM. Outside of classes, I act as the president of Geneseo‚Äôs Actuary Club and Geneseo‚Äôs chapter of the Association for Women in Mathematics. I am also the Assistant Resident Director of Onondaga South Hall."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Laya Davis",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. in Applied Mathematics | Aug 2021 - May 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Laya Davis",
    "section": "Experience",
    "text": "Experience\nExcellus BCBS | Actuary Intern | June 2023 - Aug 2023\nMath Learning Center | Tutor | January 2022 - Present"
  },
  {
    "objectID": "posts/starwars/starwars_df.html",
    "href": "posts/starwars/starwars_df.html",
    "title": "Starwars",
    "section": "",
    "text": "Let‚Äôs analyze the starwars data:\nstarwars &lt;- read_csv(\"https://bcdanl.github.io/data/starwars.csv\")"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "href": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "title": "Starwars",
    "section": "Variable Description for starwars data.frame",
    "text": "Variable Description for starwars data.frame\nThe following describes the variables in the starwars data.frame.\n\nfilms List of films the character appeared in\nname Name of the character\nspecies Name of species\nheight Height (cm)\nmass Weight (kg)\nhair_color, skin_color, eye_color Hair, skin, and eye colors\nbirth_year Year born (BBY = Before Battle of Yavin)\nsex The biological sex of the character, namely male, female, hermaphroditic, or none (as in the case for Droids).\ngender The gender role or gender identity of the character as determined by their personality or the way they were programmed (as in the case for Droids).\nhomeworld Name of homeworld"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#human-vs.-droid",
    "href": "posts/starwars/starwars_df.html#human-vs.-droid",
    "title": "Starwars",
    "section": "Human vs.¬†Droid",
    "text": "Human vs.¬†Droid\n\nggplot(data = \n         starwars %&gt;% \n         filter(species %in% c(\"Human\", \"Droid\"))) +\n  geom_boxplot(aes(x = species, y = mass, \n                   fill = species),\n               show.legend = FALSE)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code with no space in the folder name.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html",
    "href": "posts/beer-markets/beer-markets.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Let‚Äôs analyze the beer_data data:\nbeer_data &lt;- read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#variable-description-for-beer_data-data.frame",
    "href": "posts/beer-markets/beer-markets.html#variable-description-for-beer_data-data.frame",
    "title": "Beer Markets",
    "section": "Variable Description for beer_data data.frame",
    "text": "Variable Description for beer_data data.frame\nThe following describes the variables in the beer_data data.frame.\n\nhh: Household identifier\n_purchase_desc: Description of the purchase\nquantity: The quantity of beer purchased\nbrand: The brand of beer\ndollar_spent: The amount spent\nbeer_floz: Fluid ounces of beer\nprice_per_floz: Price per fluid ounce\ncontainer: Type of container\npromo: Whether the purchase was on promotion\nmarket: The market where the purchase was made\nDemographics: age, employment status, degree, class of worker (cow), race, and household information like microwave, dishwasher, tvcable, singlefamilyhome, and npeople (number of people in the household)"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#purchase-patterns",
    "href": "posts/beer-markets/beer-markets.html#purchase-patterns",
    "title": "Beer Markets",
    "section": "Purchase Patterns",
    "text": "Purchase Patterns\nWe‚Äôll explore the purchase patterns in the dataset. This includes understanding the most popular brands, the average quantity purchased, and spending habits across different markets. Here are some specific analyses we can perform:\n\nCalculate the total quantity and spending for each brand.\nFind the average quantity purchased and average spending per purchase.\nCompare the total spending across different markets.\n\nI‚Äôll begin with these analyses and create visualizations to help us understand the data better. Let‚Äôs start by calculating the total quantity and spending for each brand.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Reading the CSV file\nbeer_data = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")\n\n# Setting up the visualisation settings\nsns.set(style=\"whitegrid\")\n\n# Calculate total quantity and spending for each brand\nbrand_summary = beer_data.groupby('brand').agg({'quantity':'sum', 'dollar_spent':'sum'}).reset_index()\n\n# Sort by total quantity and spending\nbrand_summary_sorted_quantity = brand_summary.sort_values('quantity', ascending=False)\nbrand_summary_sorted_spent = brand_summary.sort_values('dollar_spent', ascending=False)\n\n\n# Plotting total quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=brand_summary_sorted_quantity, palette='viridis')\nplt.title('Total Quantity of Beer Purchased by Brand')\nplt.xlabel('Total Quantity')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n# Plotting total spending for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='dollar_spent', y='brand', data=brand_summary_sorted_spent, palette='viridis')\nplt.title('Total Spending on Beer by Brand')\nplt.xlabel('Total Spending')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\nThe bar charts above display the total quantity of beer purchased and the total spending by brand. From the looks of it, certain brands dominate in terms of quantity sold and total spending, indicating their popularity.\nNow, let‚Äôs calculate the average quantity purchased and average spending per purchase. For this, we‚Äôll consider each row in the dataset as a separate purchase and compute the averages accordingly.\n\n# Calculate average quantity purchased and average spending per purchase\naverage_purchase = beer_data.groupby('brand').agg({\n    'quantity': 'mean', \n    'dollar_spent': 'mean'\n}).reset_index()\n\n# Sort by average quantity and average spending\naverage_purchase_sorted_quantity = average_purchase.sort_values('quantity', ascending=False)\naverage_purchase_sorted_spent = average_purchase.sort_values('dollar_spent', ascending=False)\n\n# Plotting average quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=average_purchase_sorted_quantity, palette='viridis')\nplt.title('Average Quantity of Beer Purchased by Brand')\nplt.xlabel('Average Quantity')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n# Plotting average spending for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='dollar_spent', y='brand', data=average_purchase_sorted_spent, palette='viridis')\nplt.title('Average Spending on Beer by Brand')\nplt.xlabel('Average Spending')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\nThe visualizations above depict the average quantity of beer purchased per brand and the average spending per brand. This shows which brands tend to be bought in larger quantities on average and which brands tend to have higher spending per purchase, which could be indicative of their price point or the purchase of premium products.\nNext, we‚Äôll look at the total spending across different markets to see if there are any notable differences in spending habits geographically. To do this, we‚Äôll sum up the spending in each market and visualize it.\n\n# Calculate total spending in each market\nmarket_spending_summary = beer_data.groupby('market').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nmarket_spending_summary_sorted = market_spending_summary.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending in each market\nplt.figure(figsize=(12, 10))\nsns.barplot(x='dollar_spent', y='market', data=market_spending_summary_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Market')\nplt.xlabel('Total Spending')\nplt.ylabel('Market')\nplt.show()\n\n\n\n\nThe bar chart illustrates the total spending on beer by market, showcasing the differences in spending habits across various regions. Some markets have significantly higher spending, which could be due to a variety of factors including market size, consumer preferences, or economic factors.\nNow, let‚Äôs move on to the second analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "href": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "title": "Beer Markets",
    "section": "Demographic Analysis",
    "text": "Demographic Analysis\nWe will examine which demographics are buying what kind of beer and whether spending habits vary by demographics such as age, employment, and race. For this, we could look at:\n\nSpending by age group\nSpending by employment status\nSpending by race\n\nI‚Äôll start by analyzing spending by age group.\n\n# Calculate total spending by age group\nage_group_spending = beer_data.groupby('age').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nage_group_spending_sorted = age_group_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by age group\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='age', data=age_group_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Age Group')\nplt.xlabel('Total Spending')\nplt.ylabel('Age Group')\nplt.show()\n\n\n\n\nThe bar chart demonstrates the total spending on beer segmented by age group, highlighting which age groups spend the most on beer. It appears that certain age groups are more dominant in beer spending, which may align with the purchasing power or preferences of those groups.\nNext, we will examine spending by employment status.\n\n# Calculate total spending by employment status\nemployment_spending = beer_data.groupby('employment').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nemployment_spending_sorted = employment_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by employment status\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='employment', data=employment_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Employment Status')\nplt.xlabel('Total Spending')\nplt.ylabel('Employment Status')\nplt.show()\n\n\n\n\nThe visualization shows the total spending on beer by employment status. We can see that certain employment groups, such as full-time workers, are spending more on beer, which might be related to their disposable income.\nFinally, let‚Äôs look at spending by race to complete the demographic analysis.\n\n# Calculate total spending by race\nrace_spending = beer_data.groupby('race').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nrace_spending_sorted = race_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by race\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='race', data=race_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Race')\nplt.xlabel('Total Spending')\nplt.ylabel('Race')\nplt.show()\n\n\n\n\nThe bar chart above indicates the total spending on beer broken down by race, highlighting which racial groups account for the most beer spending within the dataset. This could reflect both the demographics of the regions where the data was collected and cultural preferences regarding beer.\nNow, let‚Äôs proceed to the third analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "href": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "title": "Beer Markets",
    "section": "Price Sensitivity",
    "text": "Price Sensitivity\nWe‚Äôll look at the price per fluid ounce and see if there are any trends or correlations with the quantity purchased or the brand popularity. To do this, we‚Äôll calculate the average price per fluid ounce for each brand and then visualize how this relates to the average quantity purchased and the total quantity purchased by brand.\nFirst, let‚Äôs calculate the average price per fluid ounce for each brand.\n\n# Calculate average price per fluid ounce for each brand\nbrand_price_sensitivity = beer_data.groupby('brand').agg({\n    'price_per_floz': 'mean', \n    'quantity': 'sum'\n}).reset_index()\n\n# Sort by price per fluid ounce\nbrand_price_sensitivity_sorted = brand_price_sensitivity.sort_values('price_per_floz', ascending=True)\n\n# Plotting average price per fluid ounce for each brand and the total quantity purchased\nfig, ax1 = plt.subplots(figsize=(12, 10))\n\ncolor = 'tab:red'\nax1.set_xlabel('Brand')\nax1.set_ylabel('Average Price per Fluid Ounce', color=color)\nax1.bar(brand_price_sensitivity_sorted['brand'], brand_price_sensitivity_sorted['price_per_floz'], color=color)\nax1.tick_params(axis='y', labelcolor=color)\nax1.set_xticklabels(brand_price_sensitivity_sorted['brand'], rotation=90)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\ncolor = 'tab:blue'\nax2.set_ylabel('Total Quantity Purchased', color=color)  # we already handled the x-label with ax1\nax2.plot(brand_price_sensitivity_sorted['brand'], brand_price_sensitivity_sorted['quantity'], color=color)\nax2.tick_params(axis='y', labelcolor=color)\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.title('Average Price per Fluid Ounce & Total Quantity Purchased by Brand')\nplt.show()\n\n\n\n\nIn the visualization, we have a bar graph showing the average price per fluid ounce for each brand (in red) and a line graph showing the total quantity purchased for each brand (in blue). This gives us a sense of whether there‚Äôs a relationship between the price and the quantity purchased. The x-axis labels are quite compressed due to the number of brands, but we can still observe trends such as whether lower-priced beers tend to be purchased in larger quantities.\nLastly, let‚Äôs move to the fourth analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#promotional-impact",
    "href": "posts/beer-markets/beer-markets.html#promotional-impact",
    "title": "Beer Markets",
    "section": "Promotional Impact",
    "text": "Promotional Impact\nWe‚Äôll assess the impact of promotions on the quantity of beer purchased. For this analysis, we can calculate the average quantity purchased with and without promotions and visualize the difference. We‚Äôll do this for each brand to see which brands are most affected by promotions.\nLet‚Äôs begin this analysis by looking at the average quantity purchased with and without promotions for each brand.\n\n# Calculate average quantity purchased with and without promotions for each brand\npromo_impact = beer_data.groupby(['brand', 'promo']).agg({'quantity':'mean'}).reset_index()\n\n# Pivot the data to have promo and non-promo side by side for each brand\npromo_impact_pivot = promo_impact.pivot(index='brand', columns='promo', values='quantity').reset_index()\npromo_impact_pivot.columns = ['brand', 'non_promo', 'promo']\n\n# Calculate the difference in average quantity purchased between promo and non-promo\npromo_impact_pivot['promo_impact'] = promo_impact_pivot['promo'] - promo_impact_pivot['non_promo']\n\n# Sort by the impact of promo\npromo_impact_pivot_sorted = promo_impact_pivot.sort_values('promo_impact', ascending=False)\n\n# Plotting the difference in average quantity purchased between promo and non-promo for each brand\nplt.figure(figsize=(12, 10))\nsns.barplot(x='promo_impact', y='brand', data=promo_impact_pivot_sorted, palette='viridis')\nplt.title('Impact of Promotions on Average Quantity Purchased by Brand')\nplt.xlabel('Difference in Average Quantity Purchased (Promo - Non-Promo)')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\nThe bar chart illustrates the impact of promotions on the average quantity of beer purchased by brand. A positive value indicates that, on average, more beer is purchased when there is a promotion compared to when there isn‚Äôt. Some brands appear to be significantly more influenced by promotions, with customers buying more when the products are on sale or promotion.\nThis comprehensive analysis has provided insights into purchase patterns, demographic preferences, price sensitivity, and the impact of promotions on beer purchases."
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Insightful Analytics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAnalysis the Relationship between ESG and Financial Data\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nLaya Davis\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\nLaya Davis\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nPython Basics\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nLaya Davis\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nBeer Markets\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nByeong-Hak Choe\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nStarwars\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nYour Name\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nYOUR NAME\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nYOUR NAME\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html",
    "href": "danl-210-quarto-reticulate.html",
    "title": "DANL 210: Data Preparation and Management",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#variables-and-data-types",
    "href": "danl-210-quarto-reticulate.html#variables-and-data-types",
    "title": "DANL 210: Data Preparation and Management",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#control-structures",
    "href": "danl-210-quarto-reticulate.html#control-structures",
    "title": "DANL 210: Data Preparation and Management",
    "section": "0.2 Control Structures",
    "text": "0.2 Control Structures\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‚Äòif statements‚Äô and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\nFive is greater than two!"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#functions",
    "href": "danl-210-quarto-reticulate.html#functions",
    "title": "DANL 210: Data Preparation and Management",
    "section": "0.3 Functions",
    "text": "0.3 Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\nHello from a function"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#lists-and-dictionaries",
    "href": "danl-210-quarto-reticulate.html#lists-and-dictionaries",
    "title": "DANL 210: Data Preparation and Management",
    "section": "0.4 Lists and Dictionaries",
    "text": "0.4 Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#import-python-libraries",
    "href": "danl-210-quarto-reticulate.html#import-python-libraries",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.1 Import Python libraries",
    "text": "1.1 Import Python libraries\n\nimport pandas as pd\n\n\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\n\n\n\nCode!\noj\n\n\n         sales  price      brand  ad\n0       8256.0   3.87  tropicana   0\n1       6144.0   3.87  tropicana   0\n2       3840.0   3.87  tropicana   0\n3       8000.0   3.87  tropicana   0\n4       8896.0   3.87  tropicana   0\n...        ...    ...        ...  ..\n28942   2944.0   2.00  dominicks   0\n28943   4928.0   1.94  dominicks   0\n28944  13440.0   1.59  dominicks   0\n28945  55680.0   1.49  dominicks   0\n28946   7040.0   1.75  dominicks   0\n\n[28947 rows x 4 columns]\n\n\n\noj.describe()\n\n               sales         price            ad\ncount   28947.000000  28947.000000  28947.000000\nmean    17312.213356      2.282488      0.237261\nstd     27477.660437      0.648001      0.425411\nmin        64.000000      0.520000      0.000000\n25%      4864.000000      1.790000      0.000000\n50%      8384.000000      2.170000      0.000000\n75%     17408.000000      2.730000      0.000000\nmax    716416.000100      3.870000      1.000000"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#python-r-interaction",
    "href": "danl-210-quarto-reticulate.html#python-r-interaction",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.2 Python-R Interaction",
    "text": "1.2 Python-R Interaction\nBelow is using Python‚Äôs DataFrame oj to visualize using R‚Äôs ggplot\n\nlibrary(tidyverse)\n\n# Access the Python pandas DataFrame\noj &lt;- py$oj\n\n# Plot using ggplot2\nggplot(oj, aes(x = log(sales), y = log(price), \n               color = brand)) +\n  geom_point(alpha = .25) +\n  geom_smooth(method = lm) +\n  theme_minimal()\n\n\n\n\n\n1.2.1 Interactive DataFrame with R‚Äôs DT Package\n\n\n\n\n\n\n\nIn *.ipynb on Google Colab, we can use itables or just Google Colab‚Äôs default to print DataFrame.\n\n#!pip install itables\nfrom itables import init_notebook_mode, show\ninit_notebook_mode(all_interactive=False)\n\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\nshow(oj)"
  },
  {
    "objectID": "danl-210-quarto-py-only.html",
    "href": "danl-210-quarto-py-only.html",
    "title": "DANL 210: Data Preparation and Management",
    "section": "",
    "text": "reticulate::use_condaenv(\"/Users/bchoe/anaconda3\", required = TRUE)"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#variables-and-data-types",
    "href": "danl-210-quarto-py-only.html#variables-and-data-types",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.1 Variables and Data Types",
    "text": "1.1 Variables and Data Types\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\n\nCode\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n\n10.5"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#control-structures",
    "href": "danl-210-quarto-py-only.html#control-structures",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.2 Control Structures",
    "text": "1.2 Control Structures\nPython supports the usual logical conditions from mathematics:\n\n\nCode\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\n\nThese conditions can be used in several ways, most commonly in ‚Äòif statements‚Äô and loops.\n\n\nCode\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\nFive is greater than two!"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#functions",
    "href": "danl-210-quarto-py-only.html#functions",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.3 Functions",
    "text": "1.3 Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n\nCode\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\nHello from a function"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#lists-and-dictionaries",
    "href": "danl-210-quarto-py-only.html#lists-and-dictionaries",
    "title": "DANL 210: Data Preparation and Management",
    "section": "1.4 Lists and Dictionaries",
    "text": "1.4 Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n\nCode\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-210-quarto-py-only.html#import-python-libraries",
    "href": "danl-210-quarto-py-only.html#import-python-libraries",
    "title": "DANL 210: Data Preparation and Management",
    "section": "2.1 Import Python libraries",
    "text": "2.1 Import Python libraries\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\n\n\n\n\nCode!\noj\n\n\n\n\n\n\n\n\n\nsales\nprice\nbrand\nad\n\n\n\n\n0\n8256.0\n3.87\ntropicana\n0\n\n\n1\n6144.0\n3.87\ntropicana\n0\n\n\n2\n3840.0\n3.87\ntropicana\n0\n\n\n3\n8000.0\n3.87\ntropicana\n0\n\n\n4\n8896.0\n3.87\ntropicana\n0\n\n\n...\n...\n...\n...\n...\n\n\n28942\n2944.0\n2.00\ndominicks\n0\n\n\n28943\n4928.0\n1.94\ndominicks\n0\n\n\n28944\n13440.0\n1.59\ndominicks\n0\n\n\n28945\n55680.0\n1.49\ndominicks\n0\n\n\n28946\n7040.0\n1.75\ndominicks\n0\n\n\n\n\n28947 rows √ó 4 columns\n\n\n\n\n\nCode\noj.describe()\n\n\n\n\n\n\n\n\n\nsales\nprice\nad\n\n\n\n\ncount\n28947.000000\n28947.000000\n28947.000000\n\n\nmean\n17312.213356\n2.282488\n0.237261\n\n\nstd\n27477.660437\n0.648001\n0.425411\n\n\nmin\n64.000000\n0.520000\n0.000000\n\n\n25%\n4864.000000\n1.790000\n0.000000\n\n\n50%\n8384.000000\n2.170000\n0.000000\n\n\n75%\n17408.000000\n2.730000\n0.000000\n\n\nmax\n716416.000100\n3.870000\n1.000000\n\n\n\n\n\n\n\n\n2.1.1 Interactive DataFrame with itables library\nIn *.ipynb on Google Colab, we can use itables or just Google Colab‚Äôs default to print DataFrame.\n\n\nCode\n# !pip install itables\nfrom itables import init_notebook_mode, show\ninit_notebook_mode(all_interactive=False)\n\nshow(oj)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsales\nprice\nbrand\nad\n\n\n\n\nLoading... (need help?)"
  },
  {
    "objectID": "basic-python-intro.html",
    "href": "basic-python-intro.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "basic-python-intro.html#what-is-python",
    "href": "basic-python-intro.html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "basic-python-intro.html#variables-and-data-types",
    "href": "basic-python-intro.html#variables-and-data-types",
    "title": "Introduction to Python",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "basic-python-intro.html#control-structures",
    "href": "basic-python-intro.html#control-structures",
    "title": "Introduction to Python",
    "section": "Control Structures",
    "text": "Control Structures\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‚Äòif statements‚Äô and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "basic-python-intro.html#functions",
    "href": "basic-python-intro.html#functions",
    "title": "Introduction to Python",
    "section": "Functions",
    "text": "Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "basic-python-intro.html#lists-and-dictionaries",
    "href": "basic-python-intro.html#lists-and-dictionaries",
    "title": "Introduction to Python",
    "section": "Lists and Dictionaries",
    "text": "Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "posts/python basics/python basics.html",
    "href": "posts/python basics/python basics.html",
    "title": "Python Basics",
    "section": "",
    "text": "This posts describes the basics of Python reviewed at the beginning of our DANL 300 class.\nAn important aspect of the Python language is variables. These are used to store values such as strings, integers, lists, etc. An example of using this variables is:\n\nint_var = 10\nstr_var = \"Hi!\"\nlist_var = [1,2,3]\n\nIt is often helpful to use logical conditions when coding in Python. These operators work with the same logic as they would in a math class, but can be applied on non numerical values such as strings and lists as well.\n\n# These are not equal\n\"10\" != 10\n\n# These are equal \n\"h\" == \"h\"\n2 == 2\n\n# These are true inequalities\n10 &gt; 5\n6 &lt; 12\n8 &lt;= 13\n\n# These are all false \n\"10\" != 10\n\"h\" != \"h\"\n2 != 2\n10 &lt; 5\n\nFalse\n\n\nFunctions are another important part of coding in Python and can often be used to reduce repetition and allow for user interaction.\nBelow is an example of a function that computes the following polynomial: f(x) = x + 5\n\n# To define a function\ndef f(x):\n  return x + 5\n\n# To call the function\nf(2)\n\n7"
  },
  {
    "objectID": "posts/spotify/spotify post.html",
    "href": "posts/spotify/spotify post.html",
    "title": "Spotify",
    "section": "",
    "text": "This posts describes the basics of using counting, sorting, indexing, and filtering methods on DataFrames in Python using Pandas. This will be done by using a Spotify DataFrame:\n\nimport pandas as pd\n\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\n\nAn good first step when evaluating or manipulating DataFrames is to use the .info() function to learn about its columns and their data types:\n\nspotify.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 198005 entries, 0 to 198004\nData columns (total 7 columns):\n #   Column         Non-Null Count   Dtype \n---  ------         --------------   ----- \n 0   pid            198005 non-null  int64 \n 1   playlist_name  198005 non-null  object\n 2   pos            198005 non-null  int64 \n 3   artist_name    198005 non-null  object\n 4   track_name     198005 non-null  object\n 5   duration_ms    198005 non-null  int64 \n 6   album_name     198005 non-null  object\ndtypes: int64(3), object(4)\nmemory usage: 10.6+ MB\n\n\nDataFrames are by default, given integer indexing which means each row of observations are assigned an integer beginning with 0. It can be heelpful to replace this integer indexing with information within the DataFrame instead. For example the code below converts the index to the information in the ‚Äòartist name‚Äô column. The inplace = True is used to alter the original DataFrame directly rather than Python making a copy to alter.\n\nspotify.set_index('artist_name', inplace = True)\n\nWe can then use this new index to filter the DataFrame for select artists using the .loc() function. In this case we are filtering for Taylor Swift and Beyonc√©.\n\nspotify.loc[['Taylor Swift', 'Beyonc√©']]\n\n\n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nTaylor Swift\n29\ngroovy\n33\nLook What You Made Me Do\n211859\nLook What You Made Me Do\n\n\nTaylor Swift\n44\ntaylor swift\n0\nSafe & Sound - from The Hunger Games Soundtrack\n240066\nThe Hunger Games: Songs From District 12 And B...\n\n\nTaylor Swift\n44\ntaylor swift\n2\nCrazier\n191946\nHannah Montana The Movie\n\n\nTaylor Swift\n64\nElizabeth\n48\nSafe & Sound - from The Hunger Games Soundtrack\n240066\nThe Hunger Games: Songs From District 12 And B...\n\n\nTaylor Swift\n64\nElizabeth\n49\nEyes Open\n244586\nThe Hunger Games: Songs From District 12 And B...\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nBeyonc√©\n999984\nLake\n113\n7/11\n213506\nBEYONC√â [Platinum Edition]\n\n\nBeyonc√©\n999985\nBaby Making Music\n4\nDance for You\n377466\n4\n\n\nBeyonc√©\n999985\nBaby Making Music\n24\nRocket\n391906\nBEYONC√â [Platinum Edition]\n\n\nBeyonc√©\n999989\nPARTAY\n51\nSingle Ladies (Put a Ring on It)\n192586\nI AM...SASHA FIERCE - Platinum Edition\n\n\nBeyonc√©\n999992\nGB\n106\nIrreplaceable (Irreemplazable) - Spanish version\n227666\nB'Day Deluxe Edition\n\n\n\n\n837 rows √ó 6 columns\n\n\n\nSuppose you wanted to count the number of observations that were in this DataFrame of only Taylor Swift and Beyonc√©. You could use the .count() function with the previous line of code. This will give you the number of non-missing values for each column.\n\nspotify.loc[['Taylor Swift', 'Beyonc√©']].count()\n\npid              837\nplaylist_name    837\npos              837\ntrack_name       837\nduration_ms      837\nalbum_name       837\ndtype: int64\n\n\nPerhaphs instead of knowing the number of observations you instead wanted to sort this DataFrame by the ‚Äòtrack_name‚Äô variable. You could use the .sort_values() function.\n\nspotify.loc[['Taylor Swift', 'Beyonc√©']].sort_values('track_name')\n\n\n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nBeyonc√©\n782\nMain Playlist\n180\n***Flawless\n250960\nBEYONC√â [Platinum Edition]\n\n\nBeyonc√©\n786\nSummer 2016\n53\n***Flawless\n250960\nBEYONC√â [Platinum Edition]\n\n\nBeyonc√©\n999947\nclub\n23\n***Flawless\n250960\nBEYONC√â [Platinum Edition]\n\n\nBeyonc√©\n750\nSummer 2016\n22\n***Flawless\n250960\nBEYONC√â [Platinum Edition]\n\n\nBeyonc√©\n651\nSlay\n4\n***Flawless\n250960\nBEYONC√â [Platinum Edition]\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nTaylor Swift\n1341\nin my feels\n54\nYou Belong With Me\n231133\nFearless\n\n\nTaylor Swift\n999022\nThrowbacks\n107\nYou Belong With Me\n231133\nFearless\n\n\nTaylor Swift\n999784\nThrowback\n24\nYou Belong With Me\n231133\nFearless\n\n\nTaylor Swift\n1909\nvibin\n153\nYou're Not Sorry\n261786\nFearless\n\n\nTaylor Swift\n1828\nMy Favorites\n192\nYou're Not Sorry\n261786\nFearless\n\n\n\n\n837 rows √ó 6 columns\n\n\n\nSince we altered the original DataFrame indexing, it may be helpful to revert back to the default integer indexing using the .reset_index() function. Once again the inplace = True parameter is used to alter the DataFrame itself rather than a copy.\n\nspotify.reset_index(inplace = True)"
  },
  {
    "objectID": "posts/Final Project/Final Project.html",
    "href": "posts/Final Project/Final Project.html",
    "title": "Analysis the Relationship between ESG and Financial Data",
    "section": "",
    "text": "Introduction: Although Financial Data has been reported by major companies for quiet some time, there has been a much shorter period in which major companies have been responsible for reporting on their impacts to the environment and the communities around them. The ESG Risk Scores as well as the Controversy Levels help the public assess companies as a whole, with all of the costs and benefits. Although these numbers might not be based in financials, basic economics tells us that we must access economic activity by both its internal and external costs. The ESG data and rankings help us begin to do just that. In this post I will provide some data analysis on both the financial and ESG data of 635 companies to analysis the true value of these companies.\nQuestions I wish to answer:\n\nWhat companies have the highest and lowest average stock price?\nWhat sectors have highest and lowest average stock price?\nWhat industries have highest and lowest average stock price?\nFor the highest average stock value companies, what is the overall financial health of the company?\nHow do the successful companies compare in terms of ESG Risk Score and Controversy Level?\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nesg_df = pd.read_csv('https://raw.githubusercontent.com/layadavis/layadavis.github.io/main/esg_data.csv')\nstock_data = pd.read_csv('https://raw.githubusercontent.com/layadavis/layadavis.github.io/main/Stock_data.csv')\n\nesg_stock_df = pd.merge(esg_df, stock_data, on = 'Symbol', how = \"right\")\nesg_stock_df.drop(['Unnamed: 0_x', 'Unnamed: 0_y'], axis =1, inplace = True)\n\nTo learn more about the DataFrame before we begin our analysis, the .info() function can help learn about its columns and their data types:\n\nesg_stock_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 197796 entries, 0 to 197795\nData columns (total 16 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   Symbol             197796 non-null  object \n 1   Company Name       197796 non-null  object \n 2   Sector             197796 non-null  object \n 3   Industry           197796 non-null  object \n 4   Country            197174 non-null  object \n 5   Market Cap         197796 non-null  float64\n 6   ESG Risk Score     189399 non-null  float64\n 7   Controversy Level  175093 non-null  float64\n 8   Date               197796 non-null  object \n 9   Open               197796 non-null  float64\n 10  High               197796 non-null  float64\n 11  Low                197796 non-null  float64\n 12  Close              197796 non-null  float64\n 13  Volume             197796 non-null  int64  \n 14  Dividends          197796 non-null  float64\n 15  Stock Splits       197796 non-null  float64\ndtypes: float64(9), int64(1), object(6)\nmemory usage: 24.1+ MB\n\n\nTo obtain some basic descriptive statistics on the data in the DataFrame, we can use .describe():\n\nesg_stock_df.describe()\n\n\n\n\n\n\n\n\nMarket Cap\nESG Risk Score\nControversy Level\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\n\n\ncount\n1.977960e+05\n189399.000000\n175093.000000\n197796.000000\n197796.000000\n197796.000000\n197796.000000\n1.977960e+05\n197796.000000\n197796.000000\n\n\nmean\n7.161278e+10\n21.617077\n1.976909\n149.346905\n150.930046\n147.861750\n149.453153\n4.134482e+06\n0.008567\n0.000139\n\n\nstd\n2.266365e+11\n7.158493\n0.784736\n329.725612\n333.293063\n326.693324\n330.100237\n9.289442e+06\n0.132398\n0.016646\n\n\nmin\n1.986640e+08\n6.400000\n1.000000\n1.030000\n1.060000\n0.780000\n0.980000\n0.000000e+00\n0.000000\n0.000000\n\n\n25%\n1.099163e+10\n16.300000\n1.000000\n39.998215\n40.470001\n39.551458\n40.023048\n8.323000e+05\n0.000000\n0.000000\n\n\n50%\n2.397381e+10\n21.100000\n2.000000\n82.133376\n83.036279\n81.245830\n82.176575\n1.707600e+06\n0.000000\n0.000000\n\n\n75%\n5.889363e+10\n26.100000\n2.000000\n155.040262\n156.597651\n153.630964\n155.150021\n3.854425e+06\n0.000000\n0.000000\n\n\nmax\n3.019135e+12\n52.000000\n5.000000\n8022.919922\n8158.990234\n8010.000000\n8099.959961\n3.160112e+08\n35.000000\n4.000000\n\n\n\n\n\n\n\nTo answer question number 1 we can group the DataFrame by company, calculate the average value for each companies stock close, and sort the DataFrame by its average value for close. In the code below we use the head() and tail() attributes to obtain the companies with the largest average stock close value in df1a and the companies with the smallest average stock close value in df1b.\n\ncompanies = esg_stock_df.groupby('Company Name')\n\ndf1 = (esg_stock_df\n       .assign(Close_avg = companies['Close'].transform('mean'))\n       .drop_duplicates(subset = ['Symbol'])\n       .sort_values(['Close_avg'], ascending = False)\n       )\n\ndf1a = df1.head(10)\ndf1b = df1.tail(10)\n\ndf1a\n\n\n\n\n\n\n\n\nSymbol\nCompany Name\nSector\nIndustry\nCountry\nMarket Cap\nESG Risk Score\nControversy Level\nDate\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nClose_avg\n\n\n\n\n130620\nNVR\nNVR, Inc.\nConsumer Discretionary\nResidential Construction\nUnited States\n2.436459e+10\n20.3\n1.0\n2023-01-03 00:00:00-05:00\n4647.439941\n4690.000000\n4519.049805\n4589.319824\n24900\n0.0\n0.0\n6153.677023\n\n\n26124\nBKNG\nBooking Holdings Inc.\nConsumer Discretionary\nTravel Services\nUnited States\n1.203189e+11\n19.2\n3.0\n2023-01-03 00:00:00-05:00\n2033.625803\n2043.769887\n2000.450690\n2027.022705\n266900\n0.0\n0.0\n2948.585141\n\n\n19593\nAZO\nAutoZone, Inc.\nConsumer Discretionary\nSpecialty Retail\nUnited States\n5.097399e+10\n11.0\n2.0\n2023-01-03 00:00:00-05:00\n2441.510010\n2454.709961\n2420.320068\n2431.060059\n134500\n0.0\n0.0\n2589.000900\n\n\n40741\nCMG\nChipotle Mexican Grill, Inc.\nConsumer Discretionary\nRestaurants\nUnited States\n8.739044e+10\n20.7\n2.0\n2023-01-03 00:00:00-05:00\n1400.300049\n1403.410034\n1358.030029\n1371.130005\n321300\n0.0\n0.0\n2025.354601\n\n\n118491\nMKL\nMarkel Corporation\nFinancial Services\nInsurance - Property & Casualty\nUnited States\n1.883283e+10\n26.9\n2.0\n2023-01-03 00:00:00-05:00\n1317.000000\n1333.790039\n1306.280029\n1321.969971\n41100\n0.0\n0.0\n1403.696236\n\n\n123778\nMTD\nMettler-Toledo International Inc.\nIndustrials\nDiagnostics & Research\nSwitzerland\n2.659257e+10\n13.1\nNaN\n2023-01-03 00:00:00-05:00\n1455.189941\n1468.209961\n1430.359985\n1461.869995\n84400\n0.0\n0.0\n1288.818390\n\n\n134974\nORLY\nO'Reilly Automotive, Inc.\nConsumer Discretionary\nSpecialty Retail\nUnited States\n6.163006e+10\n12.1\n1.0\n2023-01-03 00:00:00-05:00\n842.429993\n843.880005\n833.510010\n840.719971\n455500\n0.0\n0.0\n934.843987\n\n\n169806\nTDG\nTransDigm Group Incorporated\nIndustrials\nAerospace & Defense\nUnited States\n7.001662e+10\n38.7\n2.0\n2023-01-03 00:00:00-05:00\n610.590637\n611.449089\n601.466152\n603.578491\n216100\n0.0\n0.0\n865.193037\n\n\n17105\nAVGO\nBroadcom Inc.\nTechnology\nSemiconductors\nUnited States\n6.228706e+11\nNaN\n3.0\n2023-01-03 00:00:00-05:00\n550.084430\n552.586591\n536.249542\n538.868530\n2017300\n0.0\n0.0\n862.022158\n\n\n148658\nREGN\nRegeneron Pharmaceuticals, Inc.\nHealth Care\nBiotechnology\nUnited States\n9.694171e+10\n18.0\nNaN\n2023-01-03 00:00:00-05:00\n721.859985\n732.429993\n719.349976\n720.469971\n508500\n0.0\n0.0\n816.592122\n\n\n\n\n\n\n\n\ndf1b\n\n\n\n\n\n\n\n\nSymbol\nCompany Name\nSector\nIndustry\nCountry\nMarket Cap\nESG Risk Score\nControversy Level\nDate\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\nClose_avg\n\n\n\n\n46339\nCPG\nVeren Inc.\nEnergy\nOil & Gas Exploration & Production\nCanada\n5.622942e+09\n42.4\nNaN\n2023-01-03 00:00:00-05:00\n6.598251\n6.617049\n6.090693\n6.137689\n7595800\n0.0\n0.0\n7.017990\n\n\n149902\nRIG\nTransocean Ltd.\nEnergy\nOil & Gas Drilling\nSwitzerland\n4.749011e+09\n25.5\n2.0\n2023-01-03 00:00:00-05:00\n4.500000\n4.620000\n4.250000\n4.320000\n17232100\n0.0\n0.0\n6.593762\n\n\n90812\nICL\nICL Group Ltd\nIndustrials\nAgricultural Inputs\nIsrael\n6.083937e+09\n30.5\n2.0\n2023-01-03 00:00:00-05:00\n6.951882\n6.979838\n6.728228\n6.784142\n1058800\n0.0\n0.0\n5.694339\n\n\n83659\nHBI\nHanesbrands Inc.\nConsumer Discretionary\nApparel Manufacturing\nUnited States\n1.572902e+09\n14.2\n2.0\n2023-01-03 00:00:00-05:00\n6.450000\n6.730000\n6.370000\n6.730000\n10866400\n0.0\n0.0\n4.911222\n\n\n157988\nSIRI\nSirius XM Holdings Inc.\nConsumer Discretionary\nEntertainment\nUnited States\n1.160424e+10\n16.2\n2.0\n2023-01-03 00:00:00-05:00\n5.684031\n5.703497\n5.567236\n5.635366\n16513500\n0.0\n0.0\n4.496870\n\n\n154567\nSABR\nSabre Corporation\nTechnology\nTravel Services\nUnited States\n1.062546e+09\n22.7\n2.0\n2023-01-03 00:00:00-05:00\n6.350000\n6.460000\n6.080000\n6.160000\n3809900\n0.0\n0.0\n4.220981\n\n\n43851\nCOMM\nCommScope Holding Company, Inc.\nTechnology\nCommunication Equipment\nUnited States\n1.986640e+08\n14.5\n2.0\n2023-01-03 00:00:00-05:00\n7.450000\n7.640000\n7.190000\n7.420000\n3458400\n0.0\n0.0\n4.116318\n\n\n170428\nTEF\nTelef√≥nica, S.A.\nCommunication Services\nTelecom Services\nSpain\n2.587706e+10\n17.4\n2.0\n2023-01-03 00:00:00-05:00\n3.348798\n3.367300\n3.330296\n3.358049\n1406600\n0.0\n0.0\n3.903995\n\n\n35143\nCERS\nCerus Corporation\nTechnology\nMedical Devices\nUnited States\n3.099935e+08\n22.2\nNaN\n2023-01-03 00:00:00-05:00\n3.680000\n3.790000\n3.670000\n3.740000\n1082800\n0.0\n0.0\n2.228714\n\n\n109783\nLUMN\nLumen Technologies, Inc.\nCommunication Services\nTelecom Services\nUnited States\n1.221805e+09\n26.1\n2.0\n2023-01-03 00:00:00-05:00\n5.300000\n5.410000\n5.220000\n5.370000\n15331300\n0.0\n0.0\n2.179807\n\n\n\n\n\n\n\nTo answer question 2 we have the following code to form a new DataFrame that is grouped by sector.\n\nsectors = esg_stock_df.groupby('Sector')\n\ndf2 = (esg_stock_df\n       .assign(Average_Stock_Close = sectors['Close'].transform('mean'),\n               Average_ESG_Score = sectors['ESG Risk Score'].transform('mean'))\n       [['Sector', 'Average_Stock_Close', 'Average_ESG_Score']]\n       .drop_duplicates(subset = ['Sector'])\n       .sort_values(['Average_Stock_Close'], ascending = False)\n       .reset_index(drop = True)\n       )\n       \ndf2\n\n\n\n\n\n\n\n\nSector\nAverage_Stock_Close\nAverage_ESG_Score\n\n\n\n\n0\nConsumer Discretionary\n206.279119\n19.434028\n\n\n1\nIndustrials\n179.859107\n23.619149\n\n\n2\nHealth Care\n177.215485\n22.684615\n\n\n3\nTechnology\n169.162225\n18.041667\n\n\n4\nFinancial Services\n117.246608\n21.787234\n\n\n5\nReal Estate\n92.220643\n13.685000\n\n\n6\nConsumer Staples\n83.106279\n26.566667\n\n\n7\nCommunication Services\n73.124122\n21.042857\n\n\n8\nEnergy\n67.970187\n33.189286\n\n\n9\nUtilities\n66.169397\n26.626829\n\n\n10\nBasic Materials\n54.703244\n21.287500\n\n\n11\nMiscellaneous\n38.141048\n28.300000\n\n\n\n\n\n\n\nWe can then use a bar chart to see which sectors have the highest and lowest average stock value. In the below bar graph you can see that the sector with highest average stock value is Consumer Discretionary.\n\nsns.barplot(data = df2,\n            x ='Sector',\n            y = 'Average_Stock_Close')\nplt.xticks(rotation = 80)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n [Text(0, 0, 'Consumer Discretionary'),\n  Text(1, 0, 'Industrials'),\n  Text(2, 0, 'Health Care'),\n  Text(3, 0, 'Technology'),\n  Text(4, 0, 'Financial Services'),\n  Text(5, 0, 'Real Estate'),\n  Text(6, 0, 'Consumer Staples'),\n  Text(7, 0, 'Communication Services'),\n  Text(8, 0, 'Energy'),\n  Text(9, 0, 'Utilities'),\n  Text(10, 0, 'Basic Materials'),\n  Text(11, 0, 'Miscellaneous')])\n\n\n\n\n\n\n\n\n\nWe can also see how these sectors compare in terms of the average ESG Risk Score for the companies in that sector. You can see in the bar graph below that the sector Consumer Discretionary has a relatively average value for their average ESG Risk score.\n\nsns.barplot(data = df2,\n            x ='Sector',\n            y = 'Average_ESG_Score')\nplt.xticks(rotation = 80)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n [Text(0, 0, 'Consumer Discretionary'),\n  Text(1, 0, 'Industrials'),\n  Text(2, 0, 'Health Care'),\n  Text(3, 0, 'Technology'),\n  Text(4, 0, 'Financial Services'),\n  Text(5, 0, 'Real Estate'),\n  Text(6, 0, 'Consumer Staples'),\n  Text(7, 0, 'Communication Services'),\n  Text(8, 0, 'Energy'),\n  Text(9, 0, 'Utilities'),\n  Text(10, 0, 'Basic Materials'),\n  Text(11, 0, 'Miscellaneous')])\n\n\n\n\n\n\n\n\n\nTo answer question 3 we can form a new DataFrame that is grouped by industries.\n\nindustries = esg_stock_df.groupby('Industry')\n\ndf3 = (esg_stock_df\n       .assign(Average_Stock_Close = industries['Close'].transform('mean'),\n               Average_ESG_Score = industries['ESG Risk Score'].transform('mean'))\n       [['Industry', 'Average_Stock_Close', 'Average_ESG_Score']]\n       .drop_duplicates(subset = ['Industry'])\n       .sort_values(['Average_Stock_Close'], ascending = False)\n       .reset_index(drop = True)\n       )\n\ndf3\n\n\n\n\n\n\n\n\nIndustry\nAverage_Stock_Close\nAverage_ESG_Score\n\n\n\n\n0\nResidential Construction\n1310.106527\n22.140000\n\n\n1\nSpecialty Retail\n522.518824\n15.733333\n\n\n2\nRestaurants\n445.256447\n24.528571\n\n\n3\nTravel Services\n365.265593\n23.237500\n\n\n4\nSemiconductor Equipment & Materials\n345.072318\n15.125000\n\n\n...\n...\n...\n...\n\n\n118\nOil & Gas Drilling\n22.277880\n23.900000\n\n\n119\nDepartment Stores\n19.077373\n16.466667\n\n\n120\nBroadcasting\n15.874831\n14.700000\n\n\n121\nREIT - Mortgage\n13.187839\n13.650000\n\n\n122\nREIT - Hotel & Motel\n12.208811\n13.250000\n\n\n\n\n123 rows √ó 3 columns\n\n\n\nAs a follow up to questions 2 and 3 we can also group the DataFrame by sector and industries in order to see if there is a specific relationship between them and average stock close.\n\ngroups = esg_stock_df.groupby(['Sector', 'Industry'])\n\ndf3b = (esg_stock_df\n       .assign(Close_avg = groups['Close'].transform('mean'),\n               Average_ESG_Score = groups['ESG Risk Score'].transform('mean'))\n       [['Sector', 'Industry', 'Close_avg', 'Average_ESG_Score']]\n       .drop_duplicates(subset = ['Sector', 'Industry'])\n       .sort_values(['Close_avg'], ascending = False)\n       )\n       \ndf3b\n\n\n\n\n\n\n\n\nSector\nIndustry\nClose_avg\nAverage_ESG_Score\n\n\n\n\n54425\nConsumer Discretionary\nResidential Construction\n1310.106527\n22.1400\n\n\n933\nConsumer Discretionary\nSpecialty Retail\n583.066046\n14.1625\n\n\n122223\nConsumer Discretionary\nFinancial Data & Stock Exchanges\n520.521770\n16.3000\n\n\n152079\nIndustrials\nSoftware - Application\n485.056389\n20.4000\n\n\n183179\nIndustrials\nRental & Leasing Services\n467.507887\n16.9000\n\n\n...\n...\n...\n...\n...\n\n\n6842\nReal Estate\nREIT - Mortgage\n13.187839\n13.6500\n\n\n88324\nReal Estate\nREIT - Hotel & Motel\n12.208811\n13.2500\n\n\n154567\nTechnology\nTravel Services\n11.738963\n23.8000\n\n\n130931\nIndustrials\nHousehold & Personal Products\n9.655384\n13.6000\n\n\n35143\nTechnology\nMedical Devices\n2.228714\n22.2000\n\n\n\n\n168 rows √ó 4 columns\n\n\n\nTo explore the overall financial health of the top company NVR, Inc., we can use the following DataFrames that contains the companies income statements and balance sheets.\n\nincome_statements = pd.read_csv('https://raw.githubusercontent.com/layadavis/layadavis.github.io/main/yfinance-income-stmt.csv')\n\nbalance_sheets = pd.read_csv('https://raw.githubusercontent.com/layadavis/layadavis.github.io/main/yfinance-balance-sheet.csv')\n\nWe can now isolate the data for NVR, Inc.¬†using the two DataFrames formed in this code.\n\ndf4 = (income_statements[income_statements['symbol'] == \"NVR\"]\n       .rename(columns = {'Unnamed: 0': 'Line Item',\n                          '2024-03-31 00:00:00' : '2024-03-31',\n                          '2023-12-31 00:00:00' : '2023-12-31',\n                          '2023-09-30 00:00:00' : '2023-09-30',\n                          '2023-06-30 00:00:00' : '2023-06-30',\n                          '2023-03-31 00:00:00' : '2023-03-31'})\n       [['Line Item', '2023-03-31', '2023-06-30', '2023-09-30', '2023-12-31', '2024-03-31']]\n       \n       )\n       \ndf4\n\n\n\n\n\n\n\n\nLine Item\n2023-03-31\n2023-06-30\n2023-09-30\n2023-12-31\n2024-03-31\n\n\n\n\n18532\nTax Effect Of Unusual Items\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n18533\nTax Rate For Calcs\n2.060000e-01\n1.430000e-01\n1.970000e-01\n1.528200e-01\n1.620000e-01\n\n\n18534\nNormalized EBITDA\n4.452560e+08\n4.821880e+08\n5.504160e+08\n4.951690e+08\n4.818990e+08\n\n\n18535\nNet Income From Continuing Operation Net Minor...\n3.443520e+08\n4.040270e+08\n4.331570e+08\n4.100750e+08\n3.942690e+08\n\n\n18536\nReconciled Depreciation\n4.188000e+06\n4.217000e+06\n4.180000e+06\n4.331000e+06\n4.381000e+06\n\n\n18537\nReconciled Cost Of Revenue\n1.607910e+09\n1.728146e+09\n1.902174e+09\n1.812968e+09\n1.726213e+09\n\n\n18538\nEBITDA\n4.452560e+08\n4.821880e+08\n5.504160e+08\n4.951690e+08\n4.818990e+08\n\n\n18539\nEBIT\n4.410680e+08\n4.779710e+08\n5.462360e+08\n4.908380e+08\n4.775180e+08\n\n\n18540\nNet Interest Income\n-7.258000e+06\n-6.795000e+06\n-6.896000e+06\n-6.791000e+06\n-6.826000e+06\n\n\n18541\nInterest Expense\n7.258000e+06\n6.795000e+06\n6.896000e+06\n6.791000e+06\n6.826000e+06\n\n\n18542\nNormalized Income\n3.443520e+08\n4.040270e+08\n4.331570e+08\n4.100750e+08\n3.942690e+08\n\n\n18543\nNet Income From Continuing And Discontinued Op...\n3.443520e+08\n4.040270e+08\n4.331570e+08\n4.100750e+08\n3.942690e+08\n\n\n18544\nTotal Expenses\n1.741216e+09\n1.865284e+09\n2.029025e+09\n1.947700e+09\n1.861208e+09\n\n\n18545\nTotal Operating Income As Reported\n4.127510e+08\n4.413390e+08\n5.074340e+08\n4.609310e+08\n4.483270e+08\n\n\n18546\nDiluted Average Shares\n3.447000e+06\n3.467000e+06\n3.458000e+06\n3.373000e+06\n3.387000e+06\n\n\n18547\nBasic Average Shares\n3.239000e+06\n3.263000e+06\n3.259000e+06\n3.192000e+06\n3.186000e+06\n\n\n18548\nDiluted EPS\n9.989000e+01\n1.165400e+02\n1.252600e+02\n1.215600e+02\n1.164100e+02\n\n\n18549\nBasic EPS\n1.063100e+02\n1.238400e+02\n1.329200e+02\n1.284600e+02\n1.237600e+02\n\n\n18550\nDiluted NI Availto Com Stockholders\n3.443520e+08\n4.040270e+08\n4.331570e+08\n4.100750e+08\n3.942690e+08\n\n\n18551\nNet Income Common Stockholders\n3.443520e+08\n4.040270e+08\n4.331570e+08\n4.100750e+08\n3.942690e+08\n\n\n18552\nNet Income\n3.443520e+08\n4.040270e+08\n4.331570e+08\n4.100750e+08\n3.942690e+08\n\n\n18553\nNet Income Including Noncontrolling Interests\n3.443520e+08\n4.040270e+08\n4.331570e+08\n4.100750e+08\n3.942690e+08\n\n\n18554\nNet Income Continuous Operations\n3.443520e+08\n4.040270e+08\n4.331570e+08\n4.100750e+08\n3.942690e+08\n\n\n18555\nTax Provision\n8.945800e+07\n6.714900e+07\n1.061830e+08\n7.397200e+07\n7.642300e+07\n\n\n18556\nPretax Income\n4.338100e+08\n4.711760e+08\n5.393400e+08\n4.840470e+08\n4.706920e+08\n\n\n18557\nOther Income Expense\n9.890000e+05\n1.102000e+06\n1.169000e+06\n1.189000e+06\n1.171000e+06\n\n\n18558\nOther Non Operating Income Expenses\n9.890000e+05\n1.102000e+06\n1.169000e+06\n1.189000e+06\n1.171000e+06\n\n\n18559\nNet Non Operating Interest Income Expense\n-7.258000e+06\n-6.795000e+06\n-6.896000e+06\n-6.791000e+06\n-6.826000e+06\n\n\n18560\nInterest Expense Non Operating\n7.258000e+06\n6.795000e+06\n6.896000e+06\n6.791000e+06\n6.826000e+06\n\n\n18561\nOperating Income\n4.400790e+08\n4.768690e+08\n5.450670e+08\n4.896490e+08\n4.763470e+08\n\n\n18562\nOperating Expense\n1.333060e+08\n1.371380e+08\n1.268510e+08\n1.347320e+08\n1.349950e+08\n\n\n18563\nOther Operating Expenses\n-3.294600e+07\n-3.425900e+07\n-3.991400e+07\n-4.089100e+07\n-4.086600e+07\n\n\n18564\nSelling General And Administration\n1.662520e+08\n1.713970e+08\n1.667650e+08\n1.756230e+08\n1.758610e+08\n\n\n18565\nGeneral And Administrative Expense\n1.662520e+08\n1.713970e+08\n1.667650e+08\n1.756230e+08\n1.758610e+08\n\n\n18566\nOther Gand A\n1.662520e+08\n1.713970e+08\n1.667650e+08\n1.756230e+08\n1.758610e+08\n\n\n18567\nGross Profit\n5.733850e+08\n6.140070e+08\n6.719180e+08\n6.243810e+08\n6.113420e+08\n\n\n18568\nCost Of Revenue\n1.607910e+09\n1.728146e+09\n1.902174e+09\n1.812968e+09\n1.726213e+09\n\n\n18569\nTotal Revenue\n2.181295e+09\n2.342153e+09\n2.574092e+09\n2.437349e+09\n2.337555e+09\n\n\n18570\nOperating Revenue\n2.181295e+09\n2.342153e+09\n2.574092e+09\n2.437349e+09\n2.337555e+09\n\n\n\n\n\n\n\n\ndf5  = (balance_sheets[balance_sheets['symbol'] == \"NVR\"]\n       .rename(columns = {'Unnamed: 0': 'Line Item',\n                          '2024-03-31 00:00:00' : '2024-03-31',\n                          '2023-12-31 00:00:00' : '2023-12-31',\n                          '2023-09-30 00:00:00' : '2023-09-30',\n                          '2023-06-30 00:00:00' : '2023-06-30',\n                          '2023-03-31 00:00:00' : '2023-03-31'})\n       [['Line Item', '2023-03-31', '2023-06-30', '2023-09-30', '2023-12-31', '2024-03-31']]\n       )\n\ndf5.set_index(df5.columns[0], inplace = True)\n\ndf5\n\n\n\n\n\n\n\n\n2023-03-31\n2023-06-30\n2023-09-30\n2023-12-31\n2024-03-31\n\n\nLine Item\n\n\n\n\n\n\n\n\n\nTreasury Shares Number\n1.731358e+07\n1.729479e+07\n1.734535e+07\n1.736045e+07\n1.749440e+07\n\n\nOrdinary Shares Number\n3.241750e+06\n3.260538e+06\n3.209977e+06\n3.194876e+06\n3.167625e+06\n\n\nShare Issued\n2.055533e+07\n2.055533e+07\n2.055533e+07\n2.055533e+07\n2.066203e+07\n\n\nTotal Debt\n1.013884e+09\n1.016961e+09\n1.017434e+09\n1.014299e+09\n1.008643e+09\n\n\nTangible Book Value\n3.845346e+09\n4.153263e+09\n4.172583e+09\n4.364725e+09\n4.344381e+09\n\n\nInvested Capital\n4.759773e+09\n5.067226e+09\n5.086079e+09\n5.277752e+09\n5.256935e+09\n\n\nWorking Capital\n4.351682e+09\n4.663817e+09\n4.681361e+09\n4.770972e+09\n4.834917e+09\n\n\nNet Tangible Assets\n3.845346e+09\n4.153263e+09\n4.172583e+09\n4.364725e+09\n4.344381e+09\n\n\nCapital Lease Obligations\n9.945700e+07\n1.029980e+08\n1.039380e+08\n1.012720e+08\n9.608900e+07\n\n\nCommon Stock Equity\n3.845346e+09\n4.153263e+09\n4.172583e+09\n4.364725e+09\n4.344381e+09\n\n\nTotal Capitalization\n4.759773e+09\n5.067226e+09\n5.086079e+09\n5.277752e+09\n5.256935e+09\n\n\nTotal Equity Gross Minority Interest\n3.845346e+09\n4.153263e+09\n4.172583e+09\n4.364725e+09\n4.344381e+09\n\n\nStockholders Equity\n3.845346e+09\n4.153263e+09\n4.172583e+09\n4.364725e+09\n4.344381e+09\n\n\nTreasury Stock\n1.094927e+10\n1.111642e+10\n1.158360e+10\n1.184903e+10\n1.232083e+10\n\n\nRetained Earnings\n1.211777e+10\n1.252179e+10\n1.295495e+10\n1.336502e+10\n1.375929e+10\n\n\nAdditional Paid In Capital\n2.676641e+09\n2.747687e+09\n2.801027e+09\n2.848528e+09\n2.905707e+09\n\n\nCapital Stock\n2.060000e+05\n2.060000e+05\n2.060000e+05\n2.060000e+05\n2.060000e+05\n\n\nCommon Stock\n2.060000e+05\n2.060000e+05\n2.060000e+05\n2.060000e+05\n2.060000e+05\n\n\nTotal Liabilities Net Minority Interest\n2.230559e+09\n2.113512e+09\n2.199680e+09\n2.237032e+09\n2.193776e+09\n\n\nTotal Non Current Liabilities Net Minority Interest\n1.013884e+09\n1.016961e+09\n1.017434e+09\n1.014299e+09\n1.008643e+09\n\n\nNon Current Deferred Liabilities\n3.352300e+08\n3.687630e+08\n3.553110e+08\n3.344410e+08\nNaN\n\n\nNon Current Deferred Revenue\n3.352300e+08\n3.687630e+08\n3.553110e+08\n3.344410e+08\nNaN\n\n\nLong Term Debt And Capital Lease Obligation\n1.013884e+09\n1.016961e+09\n1.017434e+09\n1.014299e+09\n1.008643e+09\n\n\nLong Term Capital Lease Obligation\n9.945700e+07\n1.029980e+08\n1.039380e+08\n1.012720e+08\n9.608900e+07\n\n\nLong Term Debt\n9.144270e+08\n9.139630e+08\n9.134960e+08\n9.130270e+08\n9.125540e+08\n\n\nCurrent Liabilities\n1.216675e+09\n1.096551e+09\n1.182246e+09\n1.222733e+09\n1.185133e+09\n\n\nCurrent Deferred Liabilities\n3.352300e+08\n3.687630e+08\n3.553110e+08\n3.344410e+08\n3.553310e+08\n\n\nCurrent Deferred Revenue\n3.352300e+08\n3.687630e+08\n3.553110e+08\n3.344410e+08\n3.553310e+08\n\n\nPayables And Accrued Expenses\n8.814450e+08\n7.277880e+08\n8.269350e+08\n8.882920e+08\n8.298020e+08\n\n\nCurrent Accrued Expenses\n4.593350e+08\n2.915630e+08\n3.862990e+08\n4.130430e+08\n3.808110e+08\n\n\nPayables\n4.221100e+08\n4.362250e+08\n4.406360e+08\n4.752490e+08\n4.489910e+08\n\n\nAccounts Payable\n4.221100e+08\n4.362250e+08\n4.406360e+08\n4.752490e+08\n4.489910e+08\n\n\nTotal Assets\n6.075905e+09\n6.266775e+09\n6.372263e+09\n6.601757e+09\n6.538157e+09\n\n\nTotal Non Current Assets\n5.075480e+08\n5.064070e+08\n5.086560e+08\n6.080520e+08\n5.181070e+08\n\n\nOther Non Current Assets\n3.543610e+08\n3.477090e+08\n3.467780e+08\n2.960580e+08\n3.588500e+08\n\n\nNon Current Deferred Assets\nNaN\nNaN\nNaN\n1.480050e+08\nNaN\n\n\nNon Current Deferred Taxes Assets\nNaN\nNaN\nNaN\n1.480050e+08\nNaN\n\n\nInvestment Properties\n5.653300e+07\n5.771100e+07\n5.874300e+07\n6.371600e+07\n6.309500e+07\n\n\nNet PPE\n9.665400e+07\n1.009870e+08\n1.031350e+08\n1.002730e+08\n9.616200e+07\n\n\nAccumulated Depreciation\nNaN\nNaN\nNaN\n-1.122400e+07\nNaN\n\n\nGross PPE\n9.665400e+07\n1.009870e+08\n1.031350e+08\n1.114970e+08\n9.616200e+07\n\n\nConstruction In Progress\n5.016670e+08\n5.167090e+08\n5.301700e+08\n5.765510e+08\nNaN\n\n\nOther Properties\n9.665400e+07\n1.009870e+08\n1.031350e+08\n9.392500e+07\n9.616200e+07\n\n\nMachinery Furniture Equipment\nNaN\nNaN\nNaN\n1.757200e+07\nNaN\n\n\nCurrent Assets\n5.568357e+09\n5.760368e+09\n5.863607e+09\n5.993705e+09\n6.020050e+09\n\n\nAssets Held For Sale Current\n3.192480e+08\n4.387560e+08\n3.257920e+08\n2.225600e+08\n3.325100e+08\n\n\nRestricted Cash\n5.783100e+07\n6.547500e+07\n6.107800e+07\n5.255000e+07\n5.563600e+07\n\n\nPrepaid Assets\n5.016670e+08\n5.167090e+08\n5.301700e+08\n5.765510e+08\n6.094070e+08\n\n\nInventory\n1.865542e+09\n2.020089e+09\n2.003773e+09\n1.950150e+09\n2.117034e+09\n\n\nFinished Goods\n1.841575e+09\n1.997675e+09\n1.985977e+09\n1.926247e+09\n2.094999e+09\n\n\nRaw Materials\n2.396700e+07\n2.241400e+07\n1.779600e+07\n2.390300e+07\n2.203500e+07\n\n\nReceivables\n2.330700e+07\n2.675700e+07\n3.387800e+07\n2.900000e+07\n3.630600e+07\n\n\nAccounts Receivable\n2.330700e+07\n2.675700e+07\n3.387800e+07\n2.900000e+07\n3.630600e+07\n\n\nCash Cash Equivalents And Short Term Investments\n2.800762e+09\n2.692582e+09\n2.908916e+09\n3.162894e+09\n2.869157e+09\n\n\nCash And Cash Equivalents\n2.800762e+09\n2.692582e+09\n2.908916e+09\n3.162894e+09\n2.869157e+09\n\n\n\n\n\n\n\nBelow is a heatmap used to show the correlation between the companies different financial measures.\n\ndf6 = df5.T[['Share Issued', \"Total Debt\", \"Stockholders Equity\", 'Retained Earnings', 'Current Liabilities', 'Current Assets']]\n\ncorr = df6.corr()\n\nplt.figure(figsize=(8, 6))\n\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n\n# Title of the heatmap\nplt.title('Correlation Heatmap with Varied Correlations')\n\n# Display the heatmap\nplt.show()\n\n\n\n\n\n\n\n\nFinally lets explore the ESG ratings of the top performing companies by comparing them to the best overall scores. From df1 we can see that NVR, Inc.¬†has an ESG Risk Score of 20.3 and a Controversy Score of 1. This indicates that the company as a whole is not very controversial which is good for stockholders. It‚Äôs ESG Risk Score seems to be relatively good as it is 202.3 on a scale from 0 to 100, with 100 being the most severe and risky of companies. From the DataFrame below, we can see that its risk score is far from the lowest.\n\ndf7 = (esg_stock_df\n       .sort_values(['ESG Risk Score', 'Controversy Level'])\n       [[\"Company Name\", 'Symbol', 'ESG Risk Score', 'Controversy Level']]\n       .drop_duplicates(subset = 'Symbol')\n       )\n\ndf7\n\n\n\n\n\n\n\n\nCompany Name\nSymbol\nESG Risk Score\nControversy Level\n\n\n\n\n72463\nFlex Ltd.\nFLEX\n6.4\n1.0\n\n\n97965\nJones Lang LaSalle Incorporated\nJLL\n6.8\n1.0\n\n\n83037\nHasbro, Inc.\nHAS\n7.1\n2.0\n\n\n192820\nWheaton Precious Metals Corp.\nWPM\n7.3\nNaN\n\n\n100453\nKeysight Technologies, Inc.\nKEYS\n7.6\nNaN\n\n\n...\n...\n...\n...\n...\n\n\n184112\nVisa Inc.\nV\nNaN\n3.0\n\n\n30789\nCitigroup Inc.\nC\nNaN\n4.0\n\n\n190332\nWells Fargo & Company\nWFC\nNaN\n5.0\n\n\n80549\nGlobal Payments Inc.\nGPN\nNaN\nNaN\n\n\n145548\nPioneer Natural Resources Company\nPXD\nNaN\nNaN\n\n\n\n\n636 rows √ó 4 columns\n\n\n\nTo see how the average value of a companies stock Close, ESG Risk Score, and Controversy Level correlate with each other we have the heatmap below. It may come to some surprise to see that there is not a lot of correlation between them all. However, it does make sense that the highest amount is found between ESG Risk Score and Controversy level.\n\ncorr = df1[['Close_avg', 'ESG Risk Score', 'Controversy Level']].corr()\n\nplt.figure(figsize=(8, 6))\n\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n\n# Title of the heatmap\nplt.title('Correlation Heatmap with Varied Correlations')\n\n# Display the heatmap\nplt.show()\n\n\n\n\n\n\n\n\nSignificance of this project: As stated previously, the risk score analysis is important when evaluating the true value of companies as well as when you are considering investing in a company. Although there has been less of a focus for companies to evaluate their external risk in the past, it is becoming more important as the information is free to the public, and a high controversy level may lead to fewer investors. It is buisness best interest to being tracking this information as well as the general public as the things factored into ESG Risk Scores directly impact everyday individuals.\nReferences:\nData collected from Yahoo! finance: https://finance.yahoo.com/\nChatGPT was used as a coding reference to improve code and debug. https://chatgpt.com/\nInformation on ESG Risk Scores found on Sustainalytics. https://www.sustainalytics.com/esg-data"
  }
]